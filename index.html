
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>IJCAI Tutorial on Trustworthiness of Interpretable Machine Learning</title>
<META NAME="DESCRIPTION" CONTENT="IJCAI Tutorial,Trustworthiness,Interpretable，Machine Learning,Quanshi Zhang,Zhanxing Zhu">

<!-- <link rel="shortcut icon" type="image/x-icon" href="figures/logo.ico" /> -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
<link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
<link href="css/style.css" rel="stylesheet" type="text/css" />

<meta property="og:type" content="IJCAI Tutorial on Trustworthiness of Interpretable Machine Learning" />
<meta property="og:title" content="IJCAI Tutorial on Trustworthiness of Interpretable Machine Learning">
<meta property="og:description" content="IJCAI Tutorial on Trustworthiness of Interpretable Machine Learning">
<meta property="og:image" content="https://ijcai20interpretability.github.io/figures/rsz_logo.jpg">
<meta property="og:url" content="https://ijcai20interpretability.github.io/">

<div id='wx_logo' style='display:none;'>
  <img src='figures/rsz_logo.jpg' />
</div>

</head>

<body>
<div class="container">
  <table border="0" align="center">
    <tr>
      <td width="700" align="center" valign="middle"><h3>IJCAI 2020 Tutorial on</h3>
      <span class="title">Trustworthiness of Interpretable Machine Learning</span></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h3>July 2020 @ Yokohama, Japan</h3></td>
    </tr>
  </table>
  <!-- <p><img src="figures/main.png" width="1000" align="middle" /></p> -->
</div>

</br>

<div class="container">
  <h2>Speakers</h2>
    <div>
      <div class="instructor">
        <a href="http://qszhang.com/" >
      <div class="instructorphoto"><img src="figures/QuanshiZhang-min.jpg"></div>
      <div>Quanshi Zhang<br><font size="2.5">@ John Hopcroft Center</font><br><font size="2.5">Shanghai Jiao Tong University</font></div>
      </a>
     </div>

     <div class="instructor">
      <a href="https://sites.google.com/view/zhanxingzhu/">
          <div class="instructorphoto"><img src="figures/zhanxingzhu-min.jpg"></div>
          <div><br>Zhanxing Zhu<br><font size="2.5"> </font> <br> <font size="2.5"> </font></div>
      </a>
    </div>

    </div>
    <p></p>
</div>

</br>

<div class="container">
  <h2>Overview</h2>
    <div class="overview">
        <p>Deep neural networks (DNNs) have no doubt brought great successes to a wide range of applications in computer vision, computational linguistics and AI. However, foundational principles underlying the DNNs’ success, the trustworthiness of DNNs, and the DNNs' resilience to adversarial attacks are still largely missing. In the scope of explainable AI, the quantification of the trustworthiness of explanations to network predictions and the analysis of the trustworthiness of DNN features become a compelling yet controversial topic. Related issues include (1) the quantification of the trustworthiness of network features, (2) the objectiveness, robustness, semantic strictness of explanations of DNNs, and (3) the semantic strictness of interpretability of explainable neural networks, etc. Rethinking the trustworthiness and fairness of existing interpretable machine learning methods is of significant values for further development of interpretable machine learning.</p>

        <p>This tutorial aims to bring together researchers, engineers as well as industrial practitioners, who concern about interpretability, safety, and reliability of artificial intelligence. This tutorial introduces a number of new findings on above issues discovered in recent papers of the speakers and some classic studies. Critical discussions on the strength and limitations of current explainable-AI algorithms provide new prospective research directions. This tutorial is expected to have profound influences on critical industrial applications such as medical diagnosis, finance, and autonomous driving.</p>
    </div>
</div>

</br>

<div class="container">
  <h2>Schedule</h2>
      <h3><strong>Jan 7th Afternoon 1.2 9:40 a.m. - 11:15 a.m. UTC</strong> </h3>
      <center>
      <table cellspacing="15">
        <tr>
          <td align="center"><strong><font size="4.5"> Speaker </font></strong></td>
          <td align="center"><strong><font size="4.5"> Topic </font></strong></td>
          <td align="center"><strong><font size="4.5"> Time </font></strong></td>
          <td align="center"><strong><font size="4.5"> Link </font></strong></td>
        </tr>
        <tr>
          <td align="center"><em>Quanshi Zhang</em></td>
          <td align="center"><strong>Efforts in Pushing XAI Towards Science</strong></td>
          <td align="center"><em>9:40 a.m. - 10:30 a.m. UTC</em></td>
          <td align="center"><a href="https://youtu.be/GJHDD7bPstk">Youtube</a>, <a href="https://us02web.zoom.us/j/87172144813?pwd=d2wxNHNaZHhJMmN3RWIwbkE0SFV4QT09">Zoom</a>, <a href="./files/zqs_tutorial.pdf">Pdf</a></td>
        </tr>
        <tr>
          <td align="center"><em>Zhanxing Zhu</em></td>
          <td align="center"><strong>The Adversarial Example and Interpretability</strong></td>
          <td align="center"><em>10:30 a.m. - 11:15 a.m. UTC</em></td>
          <td align="center"><a href="#">Youtube</a>, <a href="https://us02web.zoom.us/j/87172144813?pwd=d2wxNHNaZHhJMmN3RWIwbkE0SFV4QT09">Zoom</a></td>
        </tr>
        </table>
      </center>
</div>

</br>

<div class="containersmall">
  <p>Please contact <a href="mailto:zqs1022@sjtu.edu.cn">Quanshi Zhang</a> if you have questions.</p>
</div>

<!--<p align="center" class="acknowledgement">Last updated: Jan. 6, 2017</p>-->
</body>
</html>
